{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Word2Vec CBOW and Skipgram\n",
    "## written by: [Jehwan Kim](github.com/kreimben)\n",
    "## date: 10th Feb 2024 ~ 12th Feb 2024\n",
    "## corpus data: [kaggle](https://www.kaggle.com/datasets/junbumlee/kcbert-pretraining-corpus-korean-news-comments?resource=download)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53003f90da50f342"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from icecream import ic\n",
    "\n",
    "from src.util import *"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4a8eccd365ca442",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "According to datasets from kaggle, It is already cleaned data. So I will use it directly."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92fac49a979e9b9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Dataset example\n",
    "count = 10\n",
    "data = get_data()\n",
    "while count:\n",
    "    ic(next(data))\n",
    "    count -= 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48d02dd68884bf7a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Of course there are great library `KoNLPy`, But I found more accurate library [`soynlp`](https://github.com/lovit/soynlp) so I will use that.\n",
    "soynlp learns new words automatically and calculate words statistically.\n",
    "soynlp use `Cohesion score`, `Branching Entropy` and `Accessor Variety` internally."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5776509461ba8b05"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# corpus = DoublespaceLineCorpus(\"data.txt\", iter_sent=True)\n",
    "# len(corpus)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51dda3fca1f31eef",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # to save the object cuz it consumes a lot of times!\n",
    "# with open('data.pkl', 'wb') as f:\n",
    "#     pickle.dump(corpus, f)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a5aadd13a7a7eb5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# load data object from pickle\n",
    "# if you don't have a txt file data from kaggle, uncomment it and load the object.\n",
    "with open('data.pkl', 'rb') as f:\n",
    "    corpus = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c273df6b8dfad1f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "i = 0\n",
    "for d in corpus:\n",
    "    print(i, d)\n",
    "    i += 1\n",
    "    if i > 4:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d96b4ef74587056",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# word_extractor = WordExtractor()\n",
    "# word_extractor.train(corpus)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "849b6bcc9e2e95a5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # to save the trained object\n",
    "# with open('word_extractor.pkl', 'wb') as f:\n",
    "#     pickle.dump(word_extractor, f)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3c5789faace7b27"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# to load the trained object\n",
    "with open('word_extractor.pkl', 'rb') as f:\n",
    "    word_extractor = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "163bb359a9dee6c7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# to check the score of each method.\n",
    "word_score = word_extractor.extract()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48db24a451fe4734"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cohension\n",
    "Cohesion은 문자열을 글자단위로 분리하여 부분문자열(substring)을 만들 때 왼쪽부터 문맥을 증가시키면서 각 문맥이 주어졌을 때 그 다음 글자가 나올 확률을 계산하여 누적곱을 한 값이다.\n",
    "\n",
    "![](img1.png)\n",
    "\n",
    "예를 들어 “연합뉴스가”라는 문자열이 있는 경우, 각 부분문자열의 cohesion은 다음과 같다. 한 글자는 cohesion을 계산하지 않는다.\n",
    "\n",
    "![](img2.png)\n",
    "\n",
    "하나의 단어를 중간에서 나눈 경우, 다음 글자를 예측하기 쉬우므로 조건부확률의 값은 크다. 하지만 단어가 종료된 다음에 여러가지 조사나 결합어가 오는 경우에는 다양한 경우가 가능하므로 조건부확률의 값이 작아진다. 따라서 cohesion값이 가장 큰 위치가 하나의 단어를 이루고 있을 가능성이 높다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a1993798c54a138"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "word_score[\"핵\"].cohesion_forward"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76bb8222630db1b5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "word_score[\"핵실\"].cohesion_forward"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49c16772e392193f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "word_score[\"핵실험\"].cohesion_forward"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11e8d0b595ef1c98"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "word_score[\"핵실험은\"].cohesion_forward"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9725bb4e56ba4d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "word_score[\"핵실험이\"].cohesion_forward"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c847426ffcb0e8f1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Branching Entropy\n",
    "Branching Entropy는 조건부 확률의 값이 아니라 확률분포의 엔트로피값을 사용한다. \n",
    "만약 하나의 단어를 중간에서 끊으면 다음에 나올 글자는 쉽게 예측이 가능하다. \n",
    "즉, 여러가지 글자 중 특정한 하나의 글자가 확률이 높다. 따라서 엔트로피값이 0에 가까운 값으로 작아진다. \n",
    "하지만 하나의 단어가 완결되는 위치에는 다양한 조사나 결합어가 올 수 있으므로 여러가지 글자의 확률이 비슷하게 나오고 따라서 엔트로피값이 높아진다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97e611b4093f6465"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "word_score[\"핵\"].right_branching_entropy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dfb4ef3fe2d028e2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# '핵실' 다음에는 항상 '험'만 나온다.\n",
    "word_score[\"핵실\"].right_branching_entropy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa332d2e8a6be497"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "word_score[\"핵실험\"].right_branching_entropy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7af6e6dab0c9af47"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "word_score[\"핵실험은\"].right_branching_entropy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42971d7f89af352c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Accessor Variety\n",
    "Accessor Variety는 확률분포를 구하지 않고 단순히 특정 문자열 다음에 나올 수 있는 글자의 종류만 계산한다. \n",
    "글자의 종류가 많다면 엔트로피가 높아지리 것이라고 추정하는 것이다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2808b441fd96ff9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "word_score[\"핵\"].right_accessor_variety"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac81105c60d35c51"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# '핵실' 다음에는 항상 '험'만 나온다.\n",
    "word_score[\"핵실\"].right_accessor_variety"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86601d057e6a8644"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "word_score[\"핵실험\"].right_accessor_variety"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80aeac898eb0f857"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "word_score[\"핵실험은\"].right_accessor_variety"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6aee271a9da733b6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can assume the corpus is from social network and better `Max Score` tokenising for this corpus cuz it's not cultured writing.\n",
    "\n",
    "# Max Score Tokenising\n",
    "\n",
    "최대 점수 토큰화(max score tokenizing)는 띄어쓰기가 되어 있지 않는 긴 문자열에서 가능한 모든 종류의 부분문자열을 만들어서 가장 점수가 높은 것을 하나의 토큰으로 정한다. \n",
    "이 토큰을 제외하면 이 위치를 기준으로 전체 문자열이 다시 더 작은 문자열들로 나누어지는데 이 문자열들에 대해 다시 한번 가장 점수가 높은 부분문자열을 찾는 것을 반복한다. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e29a78a915c5809e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from soynlp.tokenizer import MaxScoreTokenizer\n",
    "\n",
    "scores = {word: score.cohesion_forward for word, score in word_score.items()}\n",
    "\n",
    "maxscore_tokenizer = MaxScoreTokenizer(scores=scores)\n",
    "maxscore_tokenizer.tokenize(\"안전성에문제있는스마트폰을휴대하고탑승할경우에압수한다\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2f7f2aa6bc2e46e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

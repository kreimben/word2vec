{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Word2Vec CBOW and Skipgram\n",
    "## written by: [Jehwan Kim](github.com/kreimben)\n",
    "## date: 10th Feb 2024 ~ 12th Feb 2024\n",
    "## corpus data: [kaggle](https://www.kaggle.com/datasets/junbumlee/kcbert-pretraining-corpus-korean-news-comments?resource=download)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53003f90da50f342"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import logging  # Setting up the loggings to monitor gensim\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from soynlp import DoublespaceLineCorpus\n",
    "from soynlp.tokenizer import MaxScoreTokenizer\n",
    "\n",
    "from src.util import *\n",
    "\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt='%H:%M:%S', level=logging.INFO)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4a8eccd365ca442",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Of course there are great library `KoNLPy`, But I found more accurate library [`soynlp`](https://github.com/lovit/soynlp) so I will use that.\n",
    "soynlp learns new words automatically and calculate words statistically.\n",
    "soynlp use `Cohesion score`, `Branching Entropy` and `Accessor Variety` internally."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5776509461ba8b05"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "corpus = DoublespaceLineCorpus(\"data.txt\", iter_sent=True, num_sent=1_000_000)\n",
    "len(corpus)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51dda3fca1f31eef",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%%time\n",
    "# Awesome way to extract the words. https://github.com/lovit/soynlp\n",
    "\n",
    "from soynlp.word import WordExtractor\n",
    "from soynlp.noun import LRNounExtractor_v2\n",
    "import pickle\n",
    "\n",
    "word_extractor = WordExtractor(\n",
    "    min_frequency=10,\n",
    "    min_cohesion_forward=0.05,\n",
    "    min_right_branching_entropy=0.0\n",
    ")\n",
    "\n",
    "word_extractor.train(corpus)\n",
    "words = word_extractor.extract()\n",
    "with open('words.pkl', 'wb') as handle:\n",
    "    pickle.dump(words, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c06d847c55910086",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cohesion_score = {word: score.cohesion_forward for word, score in words.items()}\n",
    "\n",
    "noun_extractor = LRNounExtractor_v2()\n",
    "nouns = noun_extractor.train_extract(corpus)  # list of str like\n",
    "\n",
    "noun_scores = {noun: score.score for noun, score in nouns.items()}\n",
    "combined_scores = {noun: score + cohesion_score.get(noun, 0)\n",
    "                   for noun, score in noun_scores.items()}\n",
    "combined_scores.update(\n",
    "    {subword: cohesion for subword, cohesion in cohesion_score.items()\n",
    "     if not (subword in combined_scores)}\n",
    ")\n",
    "# Initializing MaxScoreTokenizer with trained scores\n",
    "tokenizer = MaxScoreTokenizer(scores=combined_scores)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "180472f69f4f0ef6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenizer.tokenize('혼전임신은 미리 조심하지 못한 여자 잘못이 크다')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "849b6bcc9e2e95a5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('tokenizer.pkl', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6b8111b1212182c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from soynlp.vectorizer import BaseVectorizer\n",
    "\n",
    "vectorizer = BaseVectorizer(\n",
    "    tokenizer=tokenizer,\n",
    "    min_tf=0,\n",
    "    max_tf=10000,\n",
    "    min_df=0,\n",
    "    max_df=1.0,\n",
    "    stopwords=None,\n",
    "    lowercase=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "corpus.iter_sent = False\n",
    "x = vectorizer.fit_transform(corpus)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60d494a78236abc6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open('vectorizer.pkl', 'wb') as handle:\n",
    "    pickle.dump(vectorizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e891b9b1deaa3f57",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# to check the score of each method.\n",
    "word_score = word_extractor.extract()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48db24a451fe4734",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cohension\n",
    "Cohesion은 문자열을 글자단위로 분리하여 부분문자열(substring)을 만들 때 왼쪽부터 문맥을 증가시키면서 각 문맥이 주어졌을 때 그 다음 글자가 나올 확률을 계산하여 누적곱을 한 값이다.\n",
    "\n",
    "![](img1.png)\n",
    "\n",
    "예를 들어 “연합뉴스가”라는 문자열이 있는 경우, 각 부분문자열의 cohesion은 다음과 같다. 한 글자는 cohesion을 계산하지 않는다.\n",
    "\n",
    "![](img2.png)\n",
    "\n",
    "하나의 단어를 중간에서 나눈 경우, 다음 글자를 예측하기 쉬우므로 조건부확률의 값은 크다. 하지만 단어가 종료된 다음에 여러가지 조사나 결합어가 오는 경우에는 다양한 경우가 가능하므로 조건부확률의 값이 작아진다. 따라서 cohesion값이 가장 큰 위치가 하나의 단어를 이루고 있을 가능성이 높다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a1993798c54a138"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "word_score[\"문\"].cohesion_forward"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76bb8222630db1b5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "word_score[\"문재\"].cohesion_forward"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49c16772e392193f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "word_score[\"문재인\"].cohesion_forward"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11e8d0b595ef1c98",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "word_score[\"문재인은\"].cohesion_forward"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9725bb4e56ba4d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "word_score[\"문재인이\"].cohesion_forward"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c847426ffcb0e8f1",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Branching Entropy\n",
    "Branching Entropy는 조건부 확률의 값이 아니라 확률분포의 엔트로피값을 사용한다. \n",
    "만약 하나의 단어를 중간에서 끊으면 다음에 나올 글자는 쉽게 예측이 가능하다. \n",
    "즉, 여러가지 글자 중 특정한 하나의 글자가 확률이 높다. 따라서 엔트로피값이 0에 가까운 값으로 작아진다. \n",
    "하지만 하나의 단어가 완결되는 위치에는 다양한 조사나 결합어가 올 수 있으므로 여러가지 글자의 확률이 비슷하게 나오고 따라서 엔트로피값이 높아진다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97e611b4093f6465"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "word_score[\"문\"].right_branching_entropy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dfb4ef3fe2d028e2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# '핵실' 다음에는 항상 '험'만 나온다.\n",
    "word_score[\"문재\"].right_branching_entropy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa332d2e8a6be497",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "word_score[\"문재인\"].right_branching_entropy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7af6e6dab0c9af47",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "word_score[\"문재인은\"].right_branching_entropy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42971d7f89af352c",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Accessor Variety\n",
    "Accessor Variety는 확률분포를 구하지 않고 단순히 특정 문자열 다음에 나올 수 있는 글자의 종류만 계산한다. \n",
    "글자의 종류가 많다면 엔트로피가 높아지리 것이라고 추정하는 것이다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2808b441fd96ff9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "word_score[\"문\"].right_accessor_variety"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac81105c60d35c51",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# '핵실' 다음에는 항상 '험'만 나온다.\n",
    "word_score[\"문재\"].right_accessor_variety"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86601d057e6a8644",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "word_score[\"문재인\"].right_accessor_variety"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80aeac898eb0f857",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "word_score[\"문재인은\"].right_accessor_variety"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6aee271a9da733b6",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can assume the corpus is from social network and better `Max Score` tokenising for this corpus cuz it's not cultured writing.\n",
    "\n",
    "# Max Score Tokenising\n",
    "\n",
    "최대 점수 토큰화(max score tokenizing)는 띄어쓰기가 되어 있지 않는 긴 문자열에서 가능한 모든 종류의 부분문자열을 만들어서 가장 점수가 높은 것을 하나의 토큰으로 정한다. \n",
    "이 토큰을 제외하면 이 위치를 기준으로 전체 문자열이 다시 더 작은 문자열들로 나누어지는데 이 문자열들에 대해 다시 한번 가장 점수가 높은 부분문자열을 찾는 것을 반복한다. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e29a78a915c5809e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from src.stopwords import *\n",
    "\n",
    "# Dataset example\n",
    "count = 500_000\n",
    "data = get_data()\n",
    "tokenised_sentences = []\n",
    "\n",
    "while count:\n",
    "    if count % 10_000 == 0: print(f'{count}', end=' ')\n",
    "    temp = ltokenizer.tokenize(next(data))\n",
    "    approve = []\n",
    "    for word in temp:\n",
    "        if not word in STOP_WORDS: approve.append(word)\n",
    "    tokenised_sentences.append(approve)\n",
    "    count -= 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7aee1013e0a76616",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "len(tokenised_sentences)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b05f3daf6372b14f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8faa09ff994cacf",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%%time\n",
    "cbow_model = Word2Vec(sentences=tokenised_sentences,\n",
    "                      sg=0,\n",
    "                      hs=1,\n",
    "                      min_count=20,\n",
    "                      window=10,\n",
    "                      vector_size=100,\n",
    "                      sample=6e-5,\n",
    "                      alpha=0.03,  # initial learning rate\n",
    "                      min_alpha=0.0007,  #minimum learning rate\n",
    "                      negative=20,\n",
    "                      workers=cores - 1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85168a2935fab33e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%%time\n",
    "skipgram_model = Word2Vec(sentences=tokenised_sentences,\n",
    "                          sg=1,\n",
    "                          hs=1,\n",
    "                          min_count=20,\n",
    "                          window=10,\n",
    "                          vector_size=100,\n",
    "                          sample=6e-5,\n",
    "                          alpha=0.03,\n",
    "                          min_alpha=0.0007,\n",
    "                          negative=20,\n",
    "                          workers=cores - 1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46f852a9d930f9d8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "cbow_model.init_sims(replace=True)\n",
    "skipgram_model.init_sims(replace=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "413b04a119f7f241",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "skipgram_model.wv.most_similar(positive='문재인')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef6c45932c4868a0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "cbow_model.wv.most_similar(positive='문재인')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62d0883ab1e703e2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "cbow_model.wv.doesnt_match(['김정은', '북한', '문재인', '트럼프'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60835259f8a4f313",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "skipgram_model.wv.doesnt_match(['김정은', '북한', '문재인', '트럼프'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a5dd8fbbe558401",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "cbow_model.wv.similarity('트럼프', '김정은')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf93f297c27c4217",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "skipgram_model.wv.similarity('김정은', '북한')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9306d28d7b2090fc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "cbow_model.wv.most_similar(positive=['김정은', '남한'], negative=['문재인'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e215146eb249d36e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "skipgram_model.wv.most_similar(positive=['김정은', '남한'], negative=['문재인'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77c390066dc96383",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "cbow_model.predict_output_word(['새끼'], topn=5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f78dbace081d12",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "skipgram_model.predict_output_word(['새끼'], topn=10)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de3e969af28998b3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# save the model\n",
    "cbow_model.wv.save_word2vec_format('./model/cbow.model')\n",
    "skipgram_model.wv.save_word2vec_format('./model/skipgram.model')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a33c074bac807cb",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualisation about model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0e2b54fc6eacd3e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# 그래프에서 마이너스 폰트 깨지는 문제에 대한 대처\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "plt.rc('font', family='D2Coding')  # Set proper font\n",
    "\n",
    "\n",
    "def show_tsne(vocab_show, X_show):\n",
    "    tsne = TSNE(n_components=2)\n",
    "    X = tsne.fit_transform(X_show)\n",
    "\n",
    "    df = pd.DataFrame(X, index=vocab_show, columns=['x', 'y'])\n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(30, 20)\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.scatter(df['x'], df['y'])\n",
    "\n",
    "    for word, pos in df.iterrows():\n",
    "        ax.annotate(word, pos, fontsize=10)\n",
    "\n",
    "    plt.xlabel(\"t-SNE 특성 0\")\n",
    "    plt.ylabel(\"t-SNE 특성 1\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_pca(vocab_show, X_show):\n",
    "    # PCA 모델을 생성합니다\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(X_show)\n",
    "    # 처음 두 개의 주성분으로 숫자 데이터를 변환합니다\n",
    "    x_pca = pca.transform(X_show)\n",
    "\n",
    "    plt.figure(figsize=(30, 20))\n",
    "    plt.xlim(x_pca[:, 0].min(), x_pca[:, 0].max())\n",
    "    plt.ylim(x_pca[:, 1].min(), x_pca[:, 1].max())\n",
    "    for i in range(len(X_show)):\n",
    "        plt.text(x_pca[i, 0], x_pca[i, 1], str(vocab_show[i]),\n",
    "                 fontdict={'weight': 'bold', 'size': 9})\n",
    "    plt.xlabel(\"첫 번째 주성분\")\n",
    "    plt.ylabel(\"두 번째 주성분\")\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "def visualise(model_name):\n",
    "    model = KeyedVectors.load_word2vec_format(model_name)\n",
    "    \n",
    "    vocab = model.index_to_key\n",
    "    X = model[vocab]\n",
    "    \n",
    "    # sz개의 단어에 대해서만 시각화\n",
    "    sz = 800\n",
    "    X_show = X[:sz, :]\n",
    "    vocab_show = vocab[:sz]\n",
    "    \n",
    "    show_tsne(vocab_show, X_show)\n",
    "    show_pca(vocab_show, X_show)\n",
    "    return None"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "419f305774b6715a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "visualise('./model/cbow.model')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5faf646612298f7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "visualise('./model/skipgram.model')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e4223a4c6623eba",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "71cc27ae024f89c4",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
